{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71100451",
   "metadata": {},
   "source": [
    "## Computing FID stats on COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/zips/val2014.zip\n",
    "!unzip val2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ba45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "coco_path = pathlib.Path(\"val2014\")\n",
    "all_images = list(coco_path.glob(\"*\"))\n",
    "\n",
    "for x in all_images:\n",
    "    img = Image.open(x)\n",
    "    resized_img = TF.resize(\n",
    "        img,\n",
    "        size=(512, 512),\n",
    "    )\n",
    "    resized_img.save(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytorch_fid --device cuda:0 --save-stats val2014 val2014_512.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352e545b",
   "metadata": {},
   "source": [
    "## Computing FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0deb3703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:27<00:00,  7.29it/s]\n",
      "FID:  36.764737485079195\n"
     ]
    }
   ],
   "source": [
    "!pytorch-fid --device cuda:0 YOUR_PATH_TO_IMAGES val2014_512.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c960f",
   "metadata": {},
   "source": [
    "## Computing CLIPScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b69c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"captions_val2014.json\") as file:\n",
    "    val_captions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4e6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "subset = [x[\"caption\"] for x in random.sample(val_captions[\"annotations\"], 10000)]\n",
    "\n",
    "np.save(\"coco_prompts.npy\", subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd79b036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two individuals learning to ski along with an instructor', 'Two vases of fresh flowers sit on top of the table.', 'A man holding up a banana in front of him.', 'A blue and white motorcycle with a trunk on the back parked by a curb.', 'close up of a black cat neat a bottle of wine', 'A boy and a birthday cake with lit candles.', 'A woman is riding the horse while the crowd watches.', 'There are bowls filled with food such as strawberries and pasta.', 'a cat is sitting inside of a persons bag', 'A little girl standing in the snow holding skis in her arms.']\n",
      "100%|█████████████████████████████████████████| 313/313 [17:56<00:00,  3.44s/it]\n",
      "CLIP score 0.3040517890625001\n",
      "0.3040517890625001\n"
     ]
    }
   ],
   "source": [
    "!python clip_score.py YOUR_PATH_TO_IMAGES 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
